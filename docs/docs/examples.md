<div style="display: inline-block; line-height: 2.25em">&nbsp;</div>

# Where things have gone wrong

To make the ideas contained in the checklist more concrete, we've compiled examples of times when things have gone wrong. They're paired these with the checklist questions to help illuminate where in process ethics discussions may have helped provide a course correction.

# Table of examples

<center>Checklist Question</center> | <center>Examples of Ethical Issues</center>
--- | ---
 | <center>**Data Collection**</center>
**A.1 Informed consent**: If there are human subjects, have those subjects have given informed consent, where users clearly understand what they are consenting to and there was a mechanism in place for gathering consent? | <ul><li>[African-American men were enrolled in the Tuskagee Study on the progression of syphillis without being told the true purpose of the study or that treatment for syphillis was being withheld.](https://en.wikipedia.org/wiki/Tuskegee_syphilis_experiment)</li></ul>
**A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those? | <ul><li>[StreetBump, a smartphone app to passively detect potholes, may fail to direct public resources to areas where smartphone penetration is lower, such as lower income areas or areas with a larger elderly population.](https://hbr.org/2013/04/the-hidden-biases-in-big-data)</li><li>[Facial recognition cameras used for passport control register Asian's eyes as closed.](http://content.time.com/time/business/article/0,8599,1954643,00.html)</li></ul>
**A.3 Limit PII exposure**: Have we considered ways to to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis? | <ul><li>[Personal information on taxi drivers can be accessed in poorly anonymized taxi trips dataset released by New York City.](https://www.theguardian.com/technology/2014/jun/27/new-york-taxi-details-anonymised-data-researchers-warn)</li><li>[Netflix prize dataset of movie rankings by 500,000 customers is easily de-anonymized through cross referencing with other publicly available datasets.](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/)</li></ul>
 | <center>**Data Storage**</center>
**B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)? | <ul><li>[Personal and financial data for more than 146 million people was stolen in Equifax data breach.](https://www.nbcnews.com/news/us-news/equifax-breaks-down-just-how-bad-last-year-s-data-n872496)</li><li>[Cambridge Analytica harvested private information from over 50 million Facebook profiles without users' permission.](https://www.nytimes.com/2018/03/17/us/politics/cambridge-analytica-trump-campaign.html)</li><li>[AOL accidentally released 20 million search queries from 658,000 customers.](https://www.wired.com/2006/08/faq-aols-search-gaffe-and-you/)</li></ul>
**B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed? | <ul><li>[The EU's General Data Protection Regulation (GDPR) includes the "right to be forgotten."](https://www.eugdpr.org/the-regulation.html)</li></ul>
**B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed? | <ul><li>[FedEx exposes private information of thousands of customers after a legacy s3 server was left open without a password.](https://www.zdnet.com/article/unsecured-server-exposes-fedex-customer-records/)</li></ul>
 | <center>**Analysis**</center>
**C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)? | <ul><li>[When Apple's HealthKit came out in 2014, women couldn't track menstruation.](https://www.theverge.com/2014/9/25/6844021/apple-promised-an-expansive-health-app-so-why-cant-i-track)</li></ul>
**C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)? | <ul><li>[word2vec, trained on Google News corpus, reinforces gender stereotypes.](https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/)</li><li>[-- Related academic study.](https://arxiv.org/abs/1607.06520)</li><li>[Women are more likely to be shown lower-paying jobs than men in Google ads.](https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study)</li></ul>
**C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data? | <ul><li>[Misleading chart shown at Planned Parenthood hearing distorts actual trends of abortions vs. cancer screenings and preventative services.](https://www.politifact.com/truth-o-meter/statements/2015/oct/01/jason-chaffetz/chart-shown-planned-parenthood-hearing-misleading-/)</li></ul>
**C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis? | <ul><li>[Strava heatmap of exercise routes reveals sensitive information on military bases and spy outposts.](https://www.theguardian.com/world/2018/jan/28/fitness-tracking-app-gives-away-location-of-secret-us-army-bases)</li></ul>
**C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future? | <ul><li>[Excel error in well-known economics paper undermines justification of austerity measures.](https://www.bbc.com/news/magazine-22223190)</li></ul>
 | <center>**Modeling**</center>
**D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory? | <ul><li>[In six major cities, Amamazon's same day delivery service excludes many predominantly black neighborhoods.](https://www.bloomberg.com/graphics/2016-amazon-same-day/)</li><li>[Variables used to predict child abuse and neglect are direct measurements of poverty, unfairly targeting low-income families for child welfare scrutiny.](https://www.wired.com/story/excerpt-from-automating-inequality/)</li><li>[Criminal sentencing risk asessments don't ask directly about race or income, but other demographic factors can end up being proxies.](https://www.themarshallproject.org/2015/08/04/the-new-science-of-sentencing)</li><li>[Creditworthiness algorithms based on nontraditional criteria such as grammatic habits, preferred grocery stores, and friends' credit scores can perpetuate systemic bias.](https://www.whitecase.com/publications/insight/algorithms-and-bias-what-lenders-need-know)</li></ul>
**D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)? | <ul><li>[Google Photos tags two African-Americans as gorillas.](https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/#12bdb1fd713d)</li><li>[With COMPAS, a risk-assessment algorithm used in criminal sentencing, black defendants are more likely to be mislabeled as likely to reoffend than white defendants.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)</li><li>[-- Northpointe's rebuttal to ProPublica article.](https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html)</li><li>[-- Related academic study.](https://www.liebertpub.com/doi/pdf/10.1089/big.2016.0047)</li><li>[Google's speech recognition software doesn't recognize women's voices as well as men's.](https://www.dailydot.com/debug/google-voice-recognition-gender-bias/)</li><li>[Facial recognition software is significanty worse at identifying people with darker skin.](https://www.theregister.co.uk/2018/02/13/facial_recognition_software_is_better_at_white_men_than_black_women/)</li><li>[-- Related academic study.](http://proceedings.mlr.press/v81/buolamwini18a.html)</li><li>[Google searches involving black-sounding names are more likely to serve up ads suggestive of a criminal record than white-sounding names.](https://www.technologyreview.com/s/510646/racism-is-poisoning-online-ad-delivery-says-harvard-professor/)</li><li>[-- Related academic study.](https://arxiv.org/abs/1301.6822)</li></ul>
**D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics? | <ul><li>[Facebook seeks to optimize "time well spent", prioritizing interaction over popularity.](https://www.wired.com/story/facebook-tweaks-newsfeed-to-favor-content-from-friends-family/)</li><li>[YouTube's search autofill suggests pedophiliac phrases due to high viewership of related videos.](https://gizmodo.com/youtubes-creepy-kid-problem-was-worse-than-we-thought-1820763240)</li></ul>
**D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed? | <ul><li>[Patients with pneumonia with a history of asthma are usually admitted to the intensive care unit as they have a high risk of dying from pneumonia. Given the success of the intensive care, neural networks predicted asthmatics had a low risk of dying and could therefore be sent home. Without explanatory models to identify this issue, patients may have been sent home to die.](http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf)</li><li>[GDPR includes a "right to explanation," i.e. meaningful information on the logic underlying automated decisions.](hhttps://academic.oup.com/idpl/article/7/4/233/4762325)</li></ul>
**D.5 Communicate bias**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood? | <ul><li>[Google Flu claims to accurately predict weekly influenza activity and then misses the 2009 swine flu pandemic.](https://www.forbes.com/sites/stevensalzberg/2014/03/23/why-google-flu-is-a-failure/#6fa6a1925535)</li></ul>
 | <center>**Deployment**</center>
**E.1 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)? | <ul><li>[Software mistakes result in healthcare cuts for people with diabetes or cerebral palsy.](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy)</li></ul>
**E.2 Roll back**: Is there a way to turn off or roll back the model in production if necessary? | <ul><li>[Google "fixes" racist algorithm by removing gorillas from image-labeling technology.](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)</li></ul>
**E.3 Concept drift**: Do we test and monitor for concept drift to ensure the model remains fair over time? | <ul><li>[Sending police officers to areas of high predicted crime skews future training data collection as police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.](https://www.smithsonianmag.com/innovation/artificial-intelligence-is-now-used-predict-crime-is-it-biased-180968337/)</li><li>[-- Related academic study.](https://arxiv.org/abs/1706.09847)</li></ul>
**E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed? | <ul><li>[Microsoft's Twitter chatbot Tay quickly becomes racist.](https://www.theguardian.com/technology/2016/mar/24/microsoft-scrambles-limit-pr-damage-over-abusive-ai-bot-tay)</li><li>[Deepfakes—realistic but fake videos generated through AI—span the gammut from celebrity porn to presidential statements.](http://theweek.com/articles/777592/rise-deepfakes)</li></ul>
